(ns ^{:doc "A TrainingMaster controls how distributed training is executed in practice

In principle, a large number of different approches can be used in distributed training (synchronous vs. asynchronous, parameter vs. gradient averaging, etc).

 Each of these different approaches would be implemented as a TrainingMaster; this allows SparkDl4jMultiLayer and SparkComputationGraph to be used with different training methods.

see: https://deeplearning4j.org/doc/org/deeplearning4j/spark/api/TrainingMaster.html"}
    dl4clj.spark.api.training-master
  (:import [org.deeplearning4j.spark.api TrainingMaster]))

;; param-avg-master currently only implementer

(defn delete-temp-files!
  "Attempt to delete any temporary files generated by this TrainingMaster.

  :spark-context can be of type:
   1) org.apache.spark.api.java.JavaSparkContext
   2) org.apache.spark.SparkContext

  returns a map of the passed args plus if the attempt was successful"
  [& {:keys [master spark-context]}]
  (if (true? (.deleteTempFiles master spark-context))
    {:master master :sc spark-context :success? true}
    {:master master :sc spark-context :success? false}))

(defn execute-training!
  "Train the SparkDl4jMultiLayer with the specified data set

  :spark-mln (SparkDl4jmultilayer), the spark representation of a mln
   - implementation not yet created,
   - see: https://deeplearning4j.org/doc/org/deeplearning4j/spark/impl/multilayer/SparkDl4jMultiLayer.html

  :data-set (javaRDD), the dataset to train on
   - a dataset wrapped in an org.apache.spark.api.java.JavaRDD obj

  :return-ds? (boolean), if you want the dataset returned with the master and spark-mln
   - defaults to false"
  [& {:keys [spark-mln master data-set return-ds?]
      :or {return-ds? false}}]
  (.executeTraining master spark-mln data-set)
  (if (true? return-ds?)
    {:master master :mln spark-mln :ds data-set}
    {:master master :mln spark-mln}))

;; there are experimental fns in this interface which are not going to be implemented
;; exectureTrainingPaths, making note incase they stick around

(defn collecting-training-stats?
  "checks to see if spark training stats are being collected"
  [master]
  (.getIsCollectTrainingStats master))

(defn get-training-stats
  "returns the training stats"
  [master]
  (.getTrainingStats master))

(defn get-worker
  "returns the work instance for this training mater"
  [& {:keys [master spark-mln]}]
  (.getWorkerInstance master spark-mln))

(defn set-collection-training-stats?
  "Set whether the training statistics should be collected.
   - defaults to true

  returns the master"
  [& {:keys [master collect-stats?]
      :or {collect-stats? true}}]
  (doto master (.setCollectTrainingStats collect-stats?)))

(defn set-master-listeners!
  "Set the iteration listeners and the StatsStorageRouter for master.

  :listeners (coll), a collection of listeners
   - see: dl4clj.optimize.listeners.listeners

  :stats-storage-router (storage) (optional)
   - not yet implemented, will be in dl4clj.api.storage
   - for now, see: https://deeplearning4j.org/doc/org/deeplearning4j/api/storage/StatsStorageRouter.html

  returns master"
  [& {:keys [master stats-storage-router listeners]
      :as opts}]
  (if (contains? opts :stats-storage-router)
    (doto master (.setListeners stats-storage-router listeners))
    (doto master (.setListeners listeners))))
